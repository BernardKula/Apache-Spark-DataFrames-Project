{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apache Spark DataFrames Project-Bernard",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/k3jLAWLM6PipuwnepIQY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CoolerKula/Apache-Spark-DataFrames-Project/blob/main/Apache_Spark_DataFrames_Project_Bernard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instructions**\n",
        "\n",
        "As a Data professional, you need to perform an analysis by answering questions about\n",
        "some stock market data on Safaricom from the years 2012-2017.\n",
        "You will need to perform the following:\n",
        "**Data Importation and Exploration**\n",
        "\n",
        "● Start a spark session and load the stock file while inferring the data types.\n",
        "\n",
        "● Determine the column names\n",
        "\n",
        "● Make observations about the schema.\n",
        "\n",
        "● Show the first 5 rows\n",
        "\n",
        "● Use the describe method to learn about the data frame\n",
        "Data Preparation\n",
        "\n",
        "● Format all the data to 2 decimal places i.e. format_number()\n",
        "\n",
        "● Create a new data frame with a column called HV Ratio that is the ratio of the\n",
        "High Price versus volume of stock traded for a day\n",
        "\n",
        "**Data Analysis**\n",
        "\n",
        "● What day had the Peak High in Price?\n",
        "\n",
        "● What is the mean of the Close column?\n",
        "\n",
        "● What is the max and min of the Volume column?\n",
        "\n",
        "● How many days was the Close lower than 60 dollars?\n",
        "\n",
        "● What percentage of the time was the High greater than 80 dollars?\n",
        "\n",
        "● What is the Pearson correlation between High and Volume?\n",
        "\n",
        "● What is the max High per year?\n",
        "\n",
        "● What is the average Close for each Calendar Month?\n"
      ],
      "metadata": {
        "id": "ro4QtnKkz0U1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_22ZcUNzpFd",
        "outputId": "1bc2a79d-f94f-4a8c-80d3-d70e14918263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 35 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 47.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=f34ac85f80361aca03526ad9dae7cd194a682e4c73c74fe1ac3c3672515e129d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ],
      "source": [
        "# Installing pyspark\n",
        "# ---\n",
        "#\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we run a local spark session\n",
        "# ---\n",
        "#\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "WjpwFNt60kwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SQLContext\n",
        "\n",
        "# Pass in the SparkContext object `sc`\n",
        "sqlCtx = SQLContext(sc)\n",
        "\n",
        "# Read csv data into a DataFrame object `df`\n",
        "df = sqlCtx.read.csv(\"saf_stock.csv\")\n",
        "\n",
        "# Print the type\n",
        "print(type(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yveEcM81Jw9",
        "outputId": "5610fb62-9d4e-4a65-d584-ff698d769203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the column names\n",
        "# ---\n",
        "#\n",
        "df = sqlCtx.read.csv(\"saf_stock.csv\", header=True)\n",
        "print(df.columns)\n",
        "#df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqW2M6MTF7_h",
        "outputId": "112889ee-0f86-4e04-a72b-8fec70591b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the first 5 rows\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "7JxNx1zqJiUy",
        "outputId": "4d86b95f-c49a-4974-e3c8-1746ad454446",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
            "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
            "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
            "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
            "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
            "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|\n",
            "|2012-01-06|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|\n",
            "|2012-01-09|         59.029999|59.549999|58.919998|             59.18| 6679300|51.616215000000004|\n",
            "+----------+------------------+---------+---------+------------------+--------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the describe method to learn about the data frame Data Preparation\n",
        "df.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnhAx5bgq9Ir",
        "outputId": "01edf04c-e7f7-461f-a2cc-e3bf4eff3436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|summary|      Date|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
            "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "|  count|      1258|              1258|             1258|             1258|             1258|             1258|             1258|\n",
            "|   mean|      null| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
            "| stddev|      null|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
            "|    min|2012-01-03|56.389998999999996|        57.060001|        56.299999|        56.419998|         10010500|        50.363689|\n",
            "|    max|2016-12-30|         90.800003|        90.970001|            89.25|        90.470001|          9994400|84.91421600000001|\n",
            "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Format all the data to 2 decimal places i.e. format_number()\n",
        "\n",
        "from pyspark.sql.functions import format_number\n",
        "\n",
        "df_2_decimal=df.select(df['Date'],format_number(df['Open'].cast('float'),2).alias('Open'),\n",
        "              format_number(df['High'].cast('float'), 2).alias('High'),\n",
        "              format_number(df['Low'].cast('float'), 2).alias('Low'),\n",
        "              format_number(df['Close'].cast('float'), 2).alias('Close'),\n",
        "              df['Volume'].cast('int').alias('Volume'),\n",
        "              format_number(df['Adj Close'].cast('float'), 2).alias('Adj Close')\n",
        "              )\n",
        "\n",
        "df_2_decimal.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uclt21I2ykqE",
        "outputId": "8e5f53eb-0491-4e43-d8ba-8291eb91d271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+-----+-----+--------+---------+\n",
            "|      Date| Open| High|  Low|Close|  Volume|Adj Close|\n",
            "+----------+-----+-----+-----+-----+--------+---------+\n",
            "|2012-01-03|59.97|61.06|59.87|60.33|12668800|    52.62|\n",
            "|2012-01-04|60.21|60.35|59.47|59.71| 9593300|    52.08|\n",
            "|2012-01-05|59.35|59.62|58.37|59.42|12768200|    51.83|\n",
            "|2012-01-06|59.42|59.45|58.87|59.00| 8069400|    51.46|\n",
            "|2012-01-09|59.03|59.55|58.92|59.18| 6679300|    51.62|\n",
            "+----------+-----+-----+-----+-----+--------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a new data frame with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day\n",
        "new_df = df_2_decimal.withColumn(\"HV Ratio\",df_2_decimal['High']/df_2_decimal['Volume'])\n",
        "print(new_df.show(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbZnmbwFz01O",
        "outputId": "61229381-be86-4fa2-d40c-2d19ab858fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+-----+-----+--------+---------+--------------------+\n",
            "|      Date| Open| High|  Low|Close|  Volume|Adj Close|            HV Ratio|\n",
            "+----------+-----+-----+-----+-----+--------+---------+--------------------+\n",
            "|2012-01-03|59.97|61.06|59.87|60.33|12668800|    52.62|4.819714574387472E-6|\n",
            "|2012-01-04|60.21|60.35|59.47|59.71| 9593300|    52.08|6.290848821573389...|\n",
            "|2012-01-05|59.35|59.62|58.37|59.42|12768200|    51.83|4.669413073103491E-6|\n",
            "|2012-01-06|59.42|59.45|58.87|59.00| 8069400|    51.46|7.367338339901356E-6|\n",
            "|2012-01-09|59.03|59.55|58.92|59.18| 6679300|    51.62|8.915604928660188E-6|\n",
            "+----------+-----+-----+-----+-----+--------+---------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}